As I’ve learned while writing The Art of Computer Programming, the “right” data structure is the one that makes the essential operations of your problem almost trivial. I don’t start by asking “Which structure is best in general?” but rather “What are the exact questions I need to answer, how often, and under what constraints?” Then I let the mathematics and a few experiments guide me.

My process, in rough order:

1) Characterize the problem precisely
- List the operations: insert, delete, search, predecessor/successor, range query, k-th, merge/split, iterations, undo, etc.
- Estimate their frequencies and the expected input distribution (random, adversarial, sorted, skewed).
- Note constraints: static vs dynamic, memory limits, persistence/undo, real-time bounds, concurrency, determinism/reproducibility, and the importance of order or stability.

2) Choose the simplest representation that naturally expresses the invariants
- If you can make an invariant obvious in the data layout, much of the algorithm writes itself.
- Prefer structures whose shape mirrors the problem: tries for prefix dictionaries, adjacency lists for sparse graphs, bitsets for small fixed universes, etc.
- For small N, a plain array with a sentinel often beats elaborate machinery.

3) Do a priori analysis, including constants
- Big-O narrows the field, but constants and locality matter. Arrays are wonderfully fast because they’re simple and contiguous.
- Consider worst-case vs average-case. If the workload is adversarial or you need real-time guarantees, worst-case bounds dominate. Otherwise, amortized or average-case may suffice.
- Ask whether you need order (trees, skips, or sorted arrays) or just membership (hashing or bitsets).

4) Prototype alternatives and measure
- Build two or three plausible candidates; instrument the operations you care about. Small, honest experiments often overturn intuition.
- Look for cache behavior, branch predictability, and allocation overhead, not just operation counts.

5) Prove and simplify
- State the representation invariant and prove that each operation maintains it.
- Use techniques like sentinels and canonical forms to eliminate boundary cases.
- If the proof is painful, the representation may be wrong.

6) Keep the door open to replacement
- Separate the interface from the representation so you can upgrade from, say, a sorted array to a balanced tree if N grows.
- Avoid premature cleverness; write the clearest thing that meets the constraints, then optimize when evidence demands.

Some concrete guidance I often find useful:

- Mostly lookups, no need for order:
  - Hash table. If memory is tight or false positives are acceptable, a Bloom filter can prefilter.
- Lookups with order or range queries:
  - Balanced search tree (red–black, AVL). If locality is important, B-trees or B+ trees. If access has strong temporal locality, splay trees can be compelling.
- Small, static sets:
  - Sorted array plus binary search; superb constants and locality.
- Prefix or dictionary operations on strings:
  - Tries, often compressed (radix). In TeX, the hyphenation patterns live in a trie; it matches the problem perfectly.
- Backtracking with frequent undo/redo:
  - Carefully linked structures that support reversible updates; for exact cover I used “dancing links” (pointer gymnastics that make backtracking O(1) per change).
- Range sums/min/max on arrays:
  - Fenwick trees (BIT) or segment trees; choose based on the complexity of updates and queries.
- Graphs:
  - Sparse: adjacency lists; Dense: adjacency matrices. If algorithms favor contraction or frequent neighbor iteration, pick the representation that makes that cheap.
- Strings and substrings at scale:
  - Suffix arrays with LCP often beat suffix trees in practice due to simplicity and locality.
- Priority queues:
  - Binary heap is usually best in practice. Fibonacci heaps shine asymptotically but have hefty constants; pairing heaps can be a sweet spot.

Trade-offs I weigh explicitly:
- Time vs space; speed vs simplicity; average vs worst-case; locality vs pointer-chasing; deterministic iteration vs randomized balance.
- Code that’s easy to verify and explain often outperforms “clever” code after the third bug.

A final heuristic: try to choose a representation so that the most frequent operation is a single, obvious loop with a simple invariant. If explaining that loop to a future reader feels easy, you’ve probably found the right data structure.